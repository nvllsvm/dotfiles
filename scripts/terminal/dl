#!/usr/bin/env python3
import argparse
import asyncio
import cgi
import re
import shutil
import sys
import urllib.parse

import httpx


class ProcessError(Exception):
    def __init__(self, process, message=None):
        self.process = process
        self.message = message

    def __str__(self):
        proc = self.process

        text = f"exit {proc.returncode}"
        if self.message is not None:
            text = f"{text} - {self.message}"

        try:
            args = proc._transport._extra["subprocess"].args
        except (AttributeError, KeyError):
            pass
        else:
            text = f"{text}: {args}"
        return text


async def clipboard_contents():
    proc = await asyncio.create_subprocess_exec(
        "cbpaste", stdout=asyncio.subprocess.PIPE
    )
    stdout, _ = await proc.communicate()
    if proc.returncode:
        raise ProcessError(proc)
    return stdout


def urlextract(data):
    pattern = re.compile(r'http[s]?://[^ \\\'"]*')
    return list(pattern.findall(data))


def prepare_urls(raw_urls, urls=None, add_prefix=False, insecure=True):
    if urls is None:
        urls = []
    prefix = "http://" if insecure else "https://"
    for url in raw_urls:
        url = url.strip()
        if not url.lower().startswith(("ftp://", "http://", "https://")):
            if not add_prefix:
                continue
            url = prefix + url
        if url and url not in urls:
            urls.append(url)
    return urls


async def get_filename(url):
    async with httpx.AsyncClient() as client:
        response = await client.head(url, follow_redirects=True)
    try:
        content_disposition = response.headers["content-disposition"]
        return cgi.parse_header(content_disposition)[1]["filename"]
    except Exception:
        return


async def download(url, insecure=False):
    filename = await get_filename(url)
    if shutil.which("aria2c"):
        args = [
            "aria2c",
            "--max-connection-per-server=16",
            "--max-concurrent-downloads=20",
            "--split=20",
            "--follow-torrent=false",
        ]
        if filename:
            args.append(f"--out={filename}")
        args.extend(
            [
                "--",
                url,
            ]
        )
        proc = await asyncio.create_subprocess_exec(*args)
        await proc.communicate()
        if proc.returncode:
            raise ProcessError(proc)
    elif shutil.which("curl"):
        args = ["curl"]
        if insecure:
            args.append("--insecure")
        args.extend(["-L", "-O"])
        proc = await asyncio.create_subprocess_exec(*args)
        await proc.communicate()
        if proc.returncode:
            raise ProcessError(proc)
    else:
        raise RuntimeError("aria2c and/or curl must be installed")


async def download_ytdlp(url):
    proc = await asyncio.create_subprocess_exec(
        "yt-dlp",
        "--output",
        "%(upload_date)s %(title)s [%(id)s].mkv",
        "--embed-subs",
        "--add-metadata",
        "--merge-output-format=mkv",
        "--",
        url,
    )
    await proc.communicate()
    if proc.returncode:
        raise ProcessError(proc)


async def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("-o", "--output")
    parser.add_argument("--insecure", action="store_true")
    parser.add_argument("--yt", action="store_true")
    parser.add_argument("url", nargs="*")
    args = parser.parse_args()

    urls = prepare_urls(
        args.url,
        add_prefix=True,
    )
    if not sys.stdin.isatty():
        prepare_urls(
            sys.stdin.read().splitlines(),
            add_prefix=False,
            urls=urls,
        )
    if not urls:
        cb = await clipboard_contents()
        prepare_urls(
            cb.decode().splitlines(),
            add_prefix=False,
            urls=urls,
        )

    if args.output and len(urls) != 1:
        raise argparse.ArgumentError(
            "must specify exactly 1 url when using -o/--output"
        )

    for url in urls:
        print(url)
        parsed = urllib.parse.urlparse(url)
        hostparts = parsed.hostname.split(".")
        if hostparts[-2:] == ["youtube", "com"] or args.yt:
            await download_ytdlp(url)
        else:
            await download(url, insecure=args.insecure)


if __name__ == "__main__":
    asyncio.run(main())
